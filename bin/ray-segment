#!/usr/bin/env python

# Python standard library
import sys, os, argparse, cPickle
import subprocess as sp
import logging

# external libraries
from numpy import unique, inf
from skimage import morphology as skmorph
from scipy.ndimage import label

# local modules
from ray import imio, agglo, morpho, classify

if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='Train a classifier for agglomerative segmentation.')
    parser.add_argument('fin',
        help='The boundary probability map to be segmented, in HDF5 format ' +\
        '(group "stack" is assumed).')
    parser.add_argument('fout', 
        help='The output filename for the segmentations (use %%f-format).')
    parser.add_argument('-I', '--remove-inclusions', action='store_true',
        default=False, 
        help='Remove inclusions before segmenting and before output.')
    parser.add_argument('-F', '--feature-manager', metavar='EVAL_STR',
        default='classify.CompositeFeatureManager(children=' +\
            '[classify.MomentsFeatureManager(), ' +\
            'classify.HistogramFeatureManager(25, 0, 1, [0.1, 0.5, 0.9])]' +\
            ')',
        help='Specify the feature manager you would like to use. ' +\
        '(This HAS to match the one used during training!)')
    parser.add_argument('-k', '--classifier', metavar='CL_FILE',
        help='Use the agglomeration classifier from file CL_FILE.')
    parser.add_argument('-T', '--training-data', metavar='TR_FILE',
        type=classify.load_training_data_from_disk,
        help='Use the agglomeration training data in TR_FILE.')
    parser.add_argument('-V', '--expected-vi', action='store_true', 
        default=False,
        help='Use expected change in VI as agglomeration function.')
    parser.add_argument('-f', '--priority-function', metavar='EVAL_STR',
        default='agglo.boundary_median',
        help='Use specified priority function (when not using a classifier).')
    parser.add_argument('-N', '--num-training', metavar='UINT', type=int,
        help='Specify the number of training examples to sample.')
    parser.add_argument('-t', '--thresholds', nargs='+', default=[],
        metavar='FLOAT',
        help='Output segmentations for the specified thresholds.')
    parser.add_argument('-w', '--watershed', metavar='FILE',
        help='Load the watershed or other oversegmentation from FILE.')
    parser.add_argument('-P', '--show-progress', action='store_true', 
        default=False, help='Show a progress bar.')
    parser.add_argument('-Z', '--nozeros', action='store_false', 
        default=True, help='Disable nozeros mode.')
    parser.add_argument('-v', '--verbose', action='store_true', default=False,
        help='Print runtime information about execution.')
    parser.add_argument('-R', '--raveler-dir',
        help='Output segmentations to Raveler. Specify with %%f-format.')
    parser.add_argument('-J', '--no-graph-json', action='store_true',
        help='When outputting to Raveler, don\'t bother outputting graph.')
    parser.add_argument('-i', '--images',
        help='specify the grayscale images for Raveler export.')
    args = parser.parse_args()

    # load all the required data
    p = imio.read_image_stack(args.fin)
    im = imio.read_image_stack(args.images) if args.images is not None else None
    if args.watershed is not None:
        ws = imio.read_image_stack(args.watershed)
    else:
        ws = skmorph.watershed(p, label(p==0)[0])
    fm = eval(args.feature_manager, {}, {'classify': classify})

    # Before doing anything drastic, check that output files are writeable:
    d, _ = os.path.split(args.fout)
    if not os.access(d, os.W_OK):
        logging.critical("No write access to " + d + ". Exiting.")
        sys.exit(1)
    if args.raveler_dir is not None:
        d, _ = os.path.split(args.raveler_dir)
        while len(d) > 0:
            if os.path.exists(d) and os.access(d, os.W_OK):
                break
            elif os.path.exists(d) and not os.access(d, os.W_OK):
                logging.critical("No write access to " + d + ". Exiting.")
                sys.exit(1)
            else:
                d, _ = os.path.split(d)

    # Load training data or classifier
    if args.training_data is not None or args.classifier is not None:
        if args.classifier is not None:
            cl = classify.RandomForest()
            cl.load_from_disk(args.classifier)
        else:
            cl = classify.RandomForest().fit(args.training_data[0], 
                args.training_data[1][:,0], args.num_training)
        if args.expected_vi:
            mpf = agglo.expected_change_vi(fm, cl)
        else:
            mpf = agglo.classifier_probability(fm, cl)
    else:
        mpf = eval(args.priority_function, {}, {'agglo': agglo})

    g = agglo.Rag(ws, p, mpf, feature_manager=fm, 
                  show_progress=args.show_progress, nozeros=args.nozeros)
    if args.remove_inclusions:
        g.remove_inclusions()
        ws = g.get_segmentation()
    if args.raveler_dir is not None:
        sps = imio.raveler_serial_section_map(ws, 0, False, False)
    for t in map(float, args.thresholds):
        g.agglomerate(t)
        g.remove_inclusions()
        s = g.get_segmentation()
        imio.write_image_stack(s, (args.fout+'.lzf.h5')%t, compression='lzf')
        g.rebuild_merge_queue()
        if args.raveler_dir is not None:
            imio.raveler_output_shortcut(ws, s, im, args.raveler_dir%t, sps)
            if not args.no_graph_json:
                g.write_plaza_json(
                            os.path.join(args.raveler_dir%t, 'graph.json'))
    g.agglomerate(inf)
    u = g.get_ucm()
    imio.write_image_stack(u, (args.fout+'.ucm.lzf.h5')%inf, compression='lzf')
